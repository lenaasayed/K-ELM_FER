{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imageio as io\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize,StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import skimage.io as io\n",
    "import math\n",
    "from operator import eq\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from skimage import color\n",
    "from operator import eq\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "random_seed = 0  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "sigma=0.0008\n",
    "c=64\n",
    "reg=100\n",
    "err=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLTP(ic,arr1):\n",
    "    Ic=ic\n",
    "    arr=arr1[0]\n",
    "    for i in range(len(arr)-1):        \n",
    "        In=arr[i]\n",
    "        tao=abs(In-Ic)/(Ic+0.0001)\n",
    "        if(In>Ic+tao):\n",
    "            arr[i]=1\n",
    "        elif(In<=Ic-tao):\n",
    "            arr[i]=-1\n",
    "        else:\n",
    "            arr[i]=0\n",
    "    return arr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pos_DLTP(arr):\n",
    "    pos_arr=np.zeros((1,8), np.uint8)[0]\n",
    "    k=0\n",
    "    for i in arr:\n",
    "        if (i==1):\n",
    "            pos_arr[k]=1\n",
    "        k=k+1    \n",
    "    return  pos_arr       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neg_DLTP(arr):\n",
    "    neg_arr=np.zeros((1,8), np.uint8)[0]\n",
    "    k=0\n",
    "    for i in arr:\n",
    "        if (i==-1):\n",
    "            neg_arr[k]=1\n",
    "        k=k+1     \n",
    "    return  neg_arr       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convertion(arr):\n",
    "    value=0\n",
    "    arr_value=[1,2,4,8,16,32,64,128]\n",
    "    value=np.dot(arr,arr_value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(block,size):\n",
    "    padd=np.pad(block, (size, size), 'constant', constant_values=(0, 0))\n",
    "    return padd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetArray(block,x,y):\n",
    "    one=block[x-1][y+1]\n",
    "    two=block[x][y+1]\n",
    "    three=block[x+1][y+1]\n",
    "    four=block[x+1][y]\n",
    "    five=block[x-1][y+1]\n",
    "    six=block[x+1][y-1]\n",
    "    seven=block[x][y-1]\n",
    "    eight=block[x-1][y-1]\n",
    "    zero=block[x][y]\n",
    "    arr=[]\n",
    "    arr.append([one,two,three,four,five,six,seven,eight])\n",
    "    return zero,arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLTP_Hist(block):\n",
    "    img=padding(block,1)\n",
    "    w,l=img.shape\n",
    "    tlbphist=[]\n",
    "    neg_hist=[]\n",
    "    pos_hist=[]\n",
    "    for y in range(1,w-1):\n",
    "        for x in range(1,l-1):\n",
    "            gc,array=GetArray(img,x,y)\n",
    "            array2=array1=DLTP(gc,array)\n",
    "            neg_arr=Neg_DLTP(array1)\n",
    "            pos_arr=Pos_DLTP(array2)\n",
    "            neg_hist.append(Convertion(neg_arr))\n",
    "            pos_hist.append(Convertion(pos_arr))\n",
    "    (histpos, _) = np.histogram(np.array(pos_hist),bins=np.arange(0,255),range=(0,255))\n",
    "    histpos = histpos.astype(\"float\")\n",
    "    (histneg, _) = np.histogram(np.array(neg_hist),bins=np.arange(0,255),range=(0,255))\n",
    "    histneg = histneg.astype(\"float\")\n",
    "    return np.array(histneg).flatten(),np.array(histpos).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algorithm_DLTP(img1,blocksize=4):\n",
    "    w,l=img1.shape\n",
    "    window=w//blocksize\n",
    "    Ndltp_hist=[]\n",
    "    Pdltp_hist=[]\n",
    "    img=padding(img1,2)\n",
    "    for r in range(2,w-2,window):\n",
    "        for c in range(2,l-2,window):\n",
    "            block = img[r:r+window,c:c+window]\n",
    "            hneg,hpos= DLTP_Hist(block)\n",
    "            Pdltp_hist.append(np.array(hpos))\n",
    "            Ndltp_hist.append(np.array(hneg))\n",
    "    dltpfeature=[]        \n",
    "    dltpfeature.append(np.array(Pdltp_hist).flatten())\n",
    "    dltpfeature.append(np.array(Ndltp_hist).flatten())\n",
    "    return np.array(dltpfeature).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img  = io.imread('S010_006_00000013.png')\n",
    "# f=feature_extract(img)\n",
    "# print(np.array(f).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_DLTP(imgs):\n",
    "    gray=preprocessing1(imgs)\n",
    "    enhanced=global_contrast_normalization(gray,reg,err)\n",
    "    norm=min_max_normalize(enhanced)\n",
    "    return np.array(Algorithm_DLTP(norm,4))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing1(img):\n",
    "    resized = cv2.resize(img,(48,48), interpolation = cv2.INTER_AREA)\n",
    "    gray=color.rgb2gray(resized)\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "            label = fn.split('.')[0]\n",
    "            path = os.path.join(path_to_dataset, fn)\n",
    "            subpath=os.listdir(path)\n",
    "            for k, d in enumerate(subpath):             \n",
    "                labels.append(label)\n",
    "                imgpath = os.path.join(path, d)\n",
    "                imgs = io.imread(imgpath)\n",
    "                features.append(feature_DLTP(imgs))\n",
    "                if k > 0 and k % 10== 0:\n",
    "                    print(\"[INFO] processed {}/{}\".format(k, len(subpath)))\n",
    "    return features, labels            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train_features, test_features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_features)\n",
    "\n",
    "    train_features = scaler.transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(img):\n",
    "    return ((img-min(img.flatten()))/(max(img.flatten())-min(img.flatten())))*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_contrast_normalization(image1,reg,err):\n",
    "    image=np.array(image1)\n",
    "    mean=np.mean(image)\n",
    "    image=(image-mean)/(max(err,math.sqrt((reg)+np.mean((image-mean)**2))))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features,labels,precentage):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    return train_features, test_features, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    return le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_dataset =r'E:\\datasetemotion\\val\\val'\n",
    "# path_to_dataset =r'E:\\datasetemotion\\train\\train'\n",
    "# path_to_dataset =r'D:\\Download\\jaffedbase'\n",
    "path_to_dataset =r'E:\\emotion\\archive\\ck\\CK+48'\n",
    "img_filenames = os.listdir(path_to_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "img_filenames = os.listdir(path_to_dataset)\n",
    "print(img_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 10/135\n",
      "[INFO] processed 20/135\n",
      "[INFO] processed 30/135\n",
      "[INFO] processed 40/135\n",
      "[INFO] processed 50/135\n",
      "[INFO] processed 60/135\n",
      "[INFO] processed 70/135\n",
      "[INFO] processed 80/135\n",
      "[INFO] processed 90/135\n",
      "[INFO] processed 100/135\n",
      "[INFO] processed 110/135\n",
      "[INFO] processed 120/135\n",
      "[INFO] processed 130/135\n",
      "[INFO] processed 10/54\n",
      "[INFO] processed 20/54\n",
      "[INFO] processed 30/54\n",
      "[INFO] processed 40/54\n",
      "[INFO] processed 50/54\n",
      "[INFO] processed 10/177\n",
      "[INFO] processed 20/177\n",
      "[INFO] processed 30/177\n",
      "[INFO] processed 40/177\n",
      "[INFO] processed 50/177\n",
      "[INFO] processed 60/177\n",
      "[INFO] processed 70/177\n",
      "[INFO] processed 80/177\n",
      "[INFO] processed 90/177\n",
      "[INFO] processed 100/177\n",
      "[INFO] processed 110/177\n",
      "[INFO] processed 120/177\n",
      "[INFO] processed 130/177\n",
      "[INFO] processed 140/177\n",
      "[INFO] processed 150/177\n",
      "[INFO] processed 160/177\n",
      "[INFO] processed 170/177\n",
      "[INFO] processed 10/75\n",
      "[INFO] processed 20/75\n",
      "[INFO] processed 30/75\n",
      "[INFO] processed 40/75\n",
      "[INFO] processed 50/75\n",
      "[INFO] processed 60/75\n",
      "[INFO] processed 70/75\n",
      "[INFO] processed 10/207\n",
      "[INFO] processed 20/207\n",
      "[INFO] processed 30/207\n",
      "[INFO] processed 40/207\n",
      "[INFO] processed 50/207\n",
      "[INFO] processed 60/207\n",
      "[INFO] processed 70/207\n",
      "[INFO] processed 80/207\n",
      "[INFO] processed 90/207\n",
      "[INFO] processed 100/207\n",
      "[INFO] processed 110/207\n",
      "[INFO] processed 120/207\n",
      "[INFO] processed 130/207\n",
      "[INFO] processed 140/207\n",
      "[INFO] processed 150/207\n",
      "[INFO] processed 160/207\n",
      "[INFO] processed 170/207\n",
      "[INFO] processed 180/207\n",
      "[INFO] processed 190/207\n",
      "[INFO] processed 200/207\n",
      "[INFO] processed 10/84\n",
      "[INFO] processed 20/84\n",
      "[INFO] processed 30/84\n",
      "[INFO] processed 40/84\n",
      "[INFO] processed 50/84\n",
      "[INFO] processed 60/84\n",
      "[INFO] processed 70/84\n",
      "[INFO] processed 80/84\n",
      "[INFO] processed 10/249\n",
      "[INFO] processed 20/249\n",
      "[INFO] processed 30/249\n",
      "[INFO] processed 40/249\n",
      "[INFO] processed 50/249\n",
      "[INFO] processed 60/249\n",
      "[INFO] processed 70/249\n",
      "[INFO] processed 80/249\n",
      "[INFO] processed 90/249\n",
      "[INFO] processed 100/249\n",
      "[INFO] processed 110/249\n",
      "[INFO] processed 120/249\n",
      "[INFO] processed 130/249\n",
      "[INFO] processed 140/249\n",
      "[INFO] processed 150/249\n",
      "[INFO] processed 160/249\n",
      "[INFO] processed 170/249\n",
      "[INFO] processed 180/249\n",
      "[INFO] processed 190/249\n",
      "[INFO] processed 200/249\n",
      "[INFO] processed 210/249\n",
      "[INFO] processed 220/249\n",
      "[INFO] processed 230/249\n",
      "[INFO] processed 240/249\n",
      "Finished loading dataset.\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset. This will take time ...')\n",
    "features, labels = load_dataset()\n",
    "print('Finished loading dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels=split_data(np.array(features),np.array(labels),0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 8128)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.7)\n",
    "X_train1 = pca.fit_transform(train_features)\n",
    "X_test1 = pca.transform(test_features)\n",
    "train_features=X_train1\n",
    "test_features=X_test1\n",
    "train_labels=train_labels\n",
    "test_labels=test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 14)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X, Y=None,g=None):\n",
    "    k=rbf_kernel(X, Y, gamma=g)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KELM_Beta(sigma,c,x,y):\n",
    "    le.fit(y)\n",
    "    y = le.transform(y)\n",
    "    emotion = len(np.unique(y))\n",
    "    One_Hot_Encoding=np.zeros((emotion,emotion), np.uint8)\n",
    "    for em in range (emotion):\n",
    "        One_Hot_Encoding[em][em]=1\n",
    "    T = One_Hot_Encoding[y, :]    \n",
    "    N,d=x.shape\n",
    "    Omega=kernel(x,None,sigma)\n",
    "    I=np.eye(N)\n",
    "    Beta = np.linalg.inv((I /c) + Omega).dot(T)    \n",
    "    return Beta,One_Hot_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KELM_Output(sigma,x,x_test,Beta,One_Hot_Encoding):\n",
    "    hx=kernel(x_test,x,sigma)\n",
    "    fx= hx.dot(Beta)\n",
    "    p=np.argmax(fx,axis=1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_pred,test_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    yt = le.fit_transform(y_pred)    \n",
    "    rt = le.fit_transform(test_labels)\n",
    "    eqw = sum(map(eq, list(rt), list(y_pred)))\n",
    "    size=len(rt)\n",
    "    acc=(eqw/size)*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 97.46192893401016\n",
      "train 100.0\n"
     ]
    }
   ],
   "source": [
    "Beta,One_Hot_Encoding=KELM_Beta(sigma,c,np.array(train_features),np.array(train_labels))\n",
    "p1=KELM_Output(sigma,np.array(train_features),np.array(test_features),Beta,One_Hot_Encoding)\n",
    "print(\"test\",acc(p1,test_labels))\n",
    "\n",
    "p2=KELM_Output(sigma,np.array(train_features),np.array(train_features),Beta,One_Hot_Encoding)\n",
    "print(\"train\",acc(p2,train_labels))\n",
    "\n",
    "# test 97.46192893401016\n",
    "# train 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746192893401016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        20\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      0.90      0.95        41\n",
      "           3       1.00      1.00      1.00        15\n",
      "           4       0.93      1.00      0.96        39\n",
      "           5       0.96      1.00      0.98        22\n",
      "           6       1.00      0.98      0.99        54\n",
      "\n",
      "    accuracy                           0.97       197\n",
      "   macro avg       0.98      0.98      0.98       197\n",
      "weighted avg       0.98      0.97      0.97       197\n",
      "\n",
      "[[20  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  0]\n",
      " [ 1  0 37  0  3  0  0]\n",
      " [ 0  0  0 15  0  0  0]\n",
      " [ 0  0  0  0 39  0  0]\n",
      " [ 0  0  0  0  0 22  0]\n",
      " [ 0  0  0  0  0  1 53]]\n"
     ]
    }
   ],
   "source": [
    "test_labels1=encode(test_labels)\n",
    "print(accuracy_score(test_labels1, p1))\n",
    "print(classification_report(test_labels1, p1))\n",
    "print(confusion_matrix(test_labels1, p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 52.79187817258884\n",
    "# train 88.13775510204081\n",
    "#after normalization\n",
    "# test 53.299492385786806\n",
    "# train 89.54081632653062\n",
    "# test 82.74111675126903\n",
    "# train 100.0\n",
    "# test 97.46192893401016\n",
    "# train 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 14)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.68020304568529 %\n",
      "Train accuracy: 97.5765306122449 %\n"
     ]
    }
   ],
   "source": [
    "def KNN_classifier(kcount,train_features, train_labels):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=kcount)\n",
    "    classifier.fit(list(train_features),list(train_labels) )\n",
    "    return classifier\n",
    "\n",
    "classifier=KNN_classifier(3,train_features, train_labels)\n",
    "y_pred = classifier.predict(test_features)\n",
    "accuracy = classifier.score(test_features, test_labels)\n",
    "print('Test accuracy:', accuracy*100, '%')\n",
    "accuracyt = classifier.score(train_features, train_labels)\n",
    "print('Train accuracy:', accuracyt*100, '%')\n",
    "\n",
    "# Test accuracy: 78.68020304568529 %\n",
    "# Train accuracy: 97.5765306122449 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7868020304568528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.80      0.67        20\n",
      "    contempt       0.86      1.00      0.92         6\n",
      "     disgust       0.72      0.63      0.68        41\n",
      "        fear       0.93      0.87      0.90        15\n",
      "       happy       0.76      0.82      0.79        39\n",
      "     sadness       0.74      0.64      0.68        22\n",
      "    surprise       0.94      0.89      0.91        54\n",
      "\n",
      "    accuracy                           0.79       197\n",
      "   macro avg       0.79      0.81      0.79       197\n",
      "weighted avg       0.80      0.79      0.79       197\n",
      "\n",
      "[[16  0  0  0  2  2  0]\n",
      " [ 0  6  0  0  0  0  0]\n",
      " [ 5  0 26  0  7  2  1]\n",
      " [ 0  0  2 13  0  0  0]\n",
      " [ 2  0  3  0 32  0  2]\n",
      " [ 5  1  2  0  0 14  0]\n",
      " [ 0  0  3  1  1  1 48]]\n"
     ]
    }
   ],
   "source": [
    "# print(test_labels.flatten())\n",
    "print(accuracy_score(test_labels, y_pred))\n",
    "print(classification_report(test_labels, y_pred))\n",
    "print(confusion_matrix(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score 95.93908629441624\n",
      "Train score 100.0\n"
     ]
    }
   ],
   "source": [
    "#RBF kernel will be suitable for complex\n",
    "#scale--->gamma 1/(n+std)\n",
    "k='rbf'\n",
    "gamma = 'scale'\n",
    "c=900\n",
    "clf = SVC(C=c,kernel=k,gamma=gamma)\n",
    "clf.fit(train_features, train_labels)\n",
    "print ('Test score', clf.score(test_features, test_labels)*100 )\n",
    "print ('Train score', clf.score(train_features, train_labels)*100 )\n",
    "# Test score 43.14720812182741\n",
    "# Train score 52.67857142857143\n",
    "# Test score 95.93908629441624\n",
    "# Train score 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# joblib.dump(p, 'model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
